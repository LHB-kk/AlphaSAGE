{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "instruments = 'sp500'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from alphagen.data.expression import *\n",
    "from alphagen.models.alpha_pool import AlphaPool\n",
    "from alphagen_generic.features import *\n",
    "from gan.utils.data import get_data_by_year\n",
    "\n",
    "\n",
    "def pred_pool(capacity,data,cache):\n",
    "    from alphagen_qlib.calculator import QLibStockDataCalculator\n",
    "    pool = AlphaPool(capacity=capacity,\n",
    "                    stock_data=data,\n",
    "                    target=target,\n",
    "                    ic_lower_bound=None)\n",
    "    exprs = []\n",
    "    for key in dict(Counter(cache).most_common(capacity)):\n",
    "        exprs.append(eval(key))\n",
    "    pool.force_load_exprs(exprs)\n",
    "    pool._optimize(alpha=5e-3, lr=5e-1, n_iter=2000)\n",
    "\n",
    "    exprs = pool.exprs[:pool.size]\n",
    "    weights = pool.weights[:pool.size]\n",
    "    calculator_test = QLibStockDataCalculator(data, target)\n",
    "    ensemble_value = calculator_test.make_ensemble_alpha(exprs, weights)\n",
    "    return ensemble_value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_gp/sp500_2016_day_0\n",
      "out_gp/sp500_2017_day_0\n",
      "out_gp/sp500_2018_day_0\n"
     ]
    }
   ],
   "source": [
    "for seed in range(1):\n",
    "    for train_end in range(2016,2019):\n",
    "        for num in [1]:\n",
    "            save_dir = f'out_gp/{instruments}_{train_end}_day_{seed}' \n",
    "            print(save_dir)\n",
    "            \n",
    "            returned = get_data_by_year(\n",
    "                train_start = 2010,train_end=train_end,valid_year=train_end+1,test_year =train_end+2,\n",
    "                instruments=instruments, target=target,freq='day',\n",
    "                qlib_path = '/root/autodl-tmp/qlib_data/us_data'\n",
    "            )\n",
    "            data_all,data,data_valid,data_valid_withhead,data_test,data_test_withhead,name = returned\n",
    "            cache = json.load(open(f'{save_dir}/2.json'))['cache']\n",
    "\n",
    "            features = ['open_', 'close', 'high', 'low', 'volume', 'vwap']\n",
    "            constants = [f'Constant({v})' for v in [-30., -10., -5., -2., -1., -0.5, -0.01, 0.01, 0.5, 1., 2., 5., 10., 30.]]\n",
    "            terminals = features + constants\n",
    "\n",
    "            pred = pred_pool(num,data_all,cache)\n",
    "            pred = pred[-data_test.n_days:]\n",
    "            torch.save(pred.detach().cpu(),f\"{save_dir}/pred_{num}.pt\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from alphagen.utils.correlation import batch_pearsonr, batch_spearmanr, batch_ret, batch_sharpe_ratio, batch_max_drawdown\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def chunk_batch_spearmanr(x, y, chunk_size=100):\n",
    "    n_days = len(x)\n",
    "    spearmanr_list= []\n",
    "    for i in range(0, n_days, chunk_size):\n",
    "        spearmanr_list.append(batch_spearmanr(x[i:i+chunk_size], y[i:i+chunk_size]))\n",
    "    spearmanr_list = torch.cat(spearmanr_list, dim=0)\n",
    "    return spearmanr_list\n",
    "\n",
    "def get_tensor_metrics(x, y, risk_free_rate=0.0):\n",
    "    # Ensure tensors are 2D (days, stocks)\n",
    "    if x.dim() > 2: x = x.squeeze(-1)\n",
    "    if y.dim() > 2: y = y.squeeze(-1)\n",
    "\n",
    "    ic_s = batch_pearsonr(x, y)\n",
    "    ric_s = chunk_batch_spearmanr(x, y, chunk_size=400)\n",
    "    ret_s = batch_ret(x, y) -0.003\n",
    "\n",
    "    ic_s = torch.nan_to_num(ic_s, nan=0.)\n",
    "    ric_s = torch.nan_to_num(ric_s, nan=0.)\n",
    "    ret_s = torch.nan_to_num(ret_s, nan=0.) / 20\n",
    "    ic_s_mean = ic_s.mean().item()\n",
    "    ic_s_std = ic_s.std().item() if ic_s.std().item() > 1e-6 else 1.0\n",
    "    ric_s_mean = ric_s.mean().item()\n",
    "    ric_s_std = ric_s.std().item() if ric_s.std().item() > 1e-6 else 1.0\n",
    "    ret_s_mean = ret_s.mean().item()\n",
    "    ret_s_std = ret_s.std().item() if ret_s.std().item() > 1e-6 else 1.0\n",
    "    \n",
    "    # Calculate Sharpe Ratio and Maximum Drawdown for ret series\n",
    "    ret_sharpe = batch_sharpe_ratio(ret_s, risk_free_rate).item()\n",
    "    ret_mdd = batch_max_drawdown(ret_s).item()\n",
    "    result = dict(\n",
    "        ic=ic_s_mean,\n",
    "        ic_std=ic_s_std,\n",
    "        icir=ic_s_mean / ic_s_std,\n",
    "        ric=ric_s_mean,\n",
    "        ric_std=ric_s_std,\n",
    "        ricir=ric_s_mean / ric_s_std,\n",
    "        ret=ret_s_mean * len(ret_s) / 3,\n",
    "        ret_std=ret_s_std,\n",
    "        retir=ret_s_mean / ret_s_std,\n",
    "        ret_sharpe=ret_sharpe,\n",
    "        ret_mdd=ret_mdd,\n",
    "    )\n",
    "    return result, ret_s\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and combine result to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_197936/3091188092.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = torch.tensor(pred).float()  # 确保是 tensor\n",
      "/tmp/ipykernel_197936/3091188092.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = torch.tensor(pred).float()  # 确保是 tensor\n",
      "/tmp/ipykernel_197936/3091188092.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = torch.tensor(pred).float()  # 确保是 tensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ic    ic_std      icir       ric   ric_std     ricir       ret  \\\n",
      "Overall  0.018521  0.057792  0.320484  0.032497  0.086953  0.373727  0.196467   \n",
      "\n",
      "          ret_std     retir  ret_sharpe  ret_mdd  \n",
      "Overall  0.003796  0.117729    1.868895  0.23914  \n"
     ]
    }
   ],
   "source": [
    "# result = []\n",
    "# instruments = 'sp500'\n",
    "# for seed in range(1):\n",
    "#     for train_end in range(2016,2019):\n",
    "#             save_dir = f'out_gp/{instruments}_{train_end}_day_{seed}' \n",
    "#             returned = get_data_by_year(\n",
    "#                 train_start = 2010,train_end=train_end,valid_year=train_end+1,test_year =train_end+2,\n",
    "#                 instruments=instruments, target=target,freq='day',\n",
    "#                 qlib_path = '/root/autodl-tmp/qlib_data/us_data'\n",
    "#             )\n",
    "#             data_all,data,data_valid,data_valid_withhead,data_test,data_test_withhead,name = returned\n",
    "#             pred = torch.load(f\"{save_dir}/pred_{num}.pt\")\n",
    "            \n",
    "#             tgt = target.evaluate(data_all)[-data_test.n_days:,:].to(\"cpu\")\n",
    "#             res, ret_s = get_tensor_metrics(torch.tensor(pred), torch.tensor(tgt))\n",
    "#             print(pd.DataFrame(res,index=[\"Test\"]))\n",
    "#             np.save(f\"{save_dir}/ret_s.npy\", ret_s)\n",
    "\n",
    "\n",
    "result = []\n",
    "instruments = 'sp500'\n",
    "all_preds = []   # 收集所有预测\n",
    "all_tgts = []    # 收集所有真实目标\n",
    "\n",
    "for seed in range(1):\n",
    "    for train_end in range(2016, 2019):\n",
    "        save_dir = f'out_gp/{instruments}_{train_end}_day_{seed}'\n",
    "        returned = get_data_by_year(\n",
    "            train_start=2010,\n",
    "            train_end=train_end,\n",
    "            valid_year=train_end + 1,\n",
    "            test_year=train_end + 2,\n",
    "            instruments=instruments,\n",
    "            target=target,\n",
    "            freq='day',\n",
    "            qlib_path='/root/autodl-tmp/qlib_data/us_data'\n",
    "        )\n",
    "        data_all, data, data_valid, data_valid_withhead, data_test, data_test_withhead, name = returned\n",
    "\n",
    "        # 加载预测结果\n",
    "        pred = torch.load(f\"{save_dir}/pred_{num}.pt\")  # shape: (n_days,)\n",
    "        pred = torch.tensor(pred).float()  # 确保是 tensor\n",
    "\n",
    "        # 获取对应的真实目标\n",
    "        tgt = target.evaluate(data_all)[-data_test.n_days:, :]  # shape: (n_days, 1) 或 (n_days,)\n",
    "        tgt = tgt.to(\"cpu\").float().squeeze()  # 转为 1D，确保和 pred 一致\n",
    "\n",
    "        # 安全检查\n",
    "        assert pred.shape == tgt.shape, f\"Shape mismatch: pred={pred.shape}, tgt={tgt.shape} in {save_dir}\"\n",
    "\n",
    "        # 收集\n",
    "        all_preds.append(pred)\n",
    "        all_tgts.append(tgt)\n",
    "\n",
    "# ===== 循环结束后，统一计算 =====\n",
    "if all_preds:\n",
    "    # 拼接所有预测和目标\n",
    "    total_pred = torch.cat(all_preds, dim=0)  # shape: (total_days,)\n",
    "    total_tgt = torch.cat(all_tgts, dim=0)    # shape: (total_days,)\n",
    "\n",
    "    # 计算整体指标\n",
    "    res, ret_s = get_tensor_metrics(total_pred, total_tgt)\n",
    "    df_res = pd.DataFrame([res], index=[\"Overall\"])\n",
    "    print(df_res)\n",
    "\n",
    "    # 保存整体结果\n",
    "    np.save(\"out_gp/overall_ret_s.npy\", ret_s)\n",
    "else:\n",
    "    print(\"No data collected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
